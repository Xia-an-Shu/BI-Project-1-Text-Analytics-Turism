# -*- coding: utf-8 -*-
"""ProyectoBI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BzM1HcMsZcz9sdvujfXMr7ZqbGSZRZzv

# **Proyecto Turismo de los Alpes**

# **1. Entendimiento del negocio y enfoque analítico**

### **1.1 Oportunidad/problema Negocio**

El Ministerio de Comercio, Industria y Turismo de Colombia, junto con la Asociación Hotelera y Turística de Colombia (COTELCO) y diversas cadenas hoteleras, incluyendo Hilton, Hoteles Estelar y Holiday Inn, así como hoteles más pequeños en diferentes municipios del país, se enfrentan a la necesidad de analizar las características de los sitios turísticos que atraen a los visitantes locales y extranjeros, con el objetivo de comprender qué los hace atractivos o poco recomendables. Además, buscan establecer un mecanismo para evaluar la calidad de los sitios según las reseñas de los turistas, con la esperanza de aumentar el número de turistas que visitan los destinos analizados, mejorar la satisfacción del cliente con la experiencia turística, y posiblemente, incrementar los ingresos generados por el turismo en esas áreas.

### **1.2 Enfoque analítico**

Tenien en cuenta la problematica, el objetivo desde el punto de vista de aprendizaje automático es desarrollar un modelo capaz de clasificar nuevas reseñas en la escala proporcionada (de 1 a 5) donde 5 es que la reseña es muy positiva y 1 es que es muy negativa. Para ello, se propone realizar lo siguiente:

1. Preprocesamiento de datos:

 Para trabajar con los algoritmos de aprendizaje de maquina se deberá eliminar los signos de puntuación, caracteres especiales y convertir todo el texto a minúsculas. Luego, se eliminará palabras vacías (stop words) y se hará una lematización o stemming para reducir las palabras a su forma base. Por último, se hará una vectorización del texto mediante la técnica de TF-IDF para convertir las palabras en vectores numéricos ponderados según su importancia en el contexto de la reseña.

2. Selección de los algoritmos:

 Primer Algoritmo: Dado que se trata de un problema de clasificación multiclase con datos textuales, se va a utilizar el de clasificador de Regresión Logística Multinomial. Este algoritmo es adecuado porque es versátil y eficiente, especialmente diseñado para manejar datos textuales y puede manejar múltiples categorías de salida. Además, ya que la técnica TF-IDF permite representar el texto de manera eficiente, capturando la importancia relativa de las palabras en cada documento, permite que esta combinación ofrezca un equilibrio entre simplicidad, eficacia y capacidad para manejar datos textuales, lo que lo convierte en una elección sólida para este proyecto.

 Segundo Algoritmo: Los árboles de decisión son una opción adecuada para clasificar reseñas turísticas debido a su capacidad para aprender relaciones no lineales entre las características del texto y las calificaciones asociadas. Utilizan una estructura jerárquica de reglas de decisión que facilita la interpretación y comprensión de cómo se clasifican las reseñas. Además, son capaces de manejar datos categóricos y numéricos, lo que los hace versátiles para el procesamiento de texto.

 Tercer Algoritmo:

4. Creación del modelo:

 Para la implemetación de cada algoritmo primero se va a dividir los datos en conjuntos de entrenamiento y prueba. Luego, se entrenará el modelo utilizando el conjunto de entrenamiento y se ajustará los hiperparámetros para mejorar el rendimiento del modelo.

5. Validación del modelo:

  Se evaluará el rendimiento de los modelos utilizando métricas adecuadas para la clasificación multiclase, como la precisión, el recall, la puntuación F1 y la matriz de confusión. Después, se elegira al mejor de los 3 generados y se analizará las métricas para comprender cómo el modelo seleccionado está clasificando las reseñas en las diferentes categorías de la escala.

Para el primer enfoque, utilizaremos Regresión Logística Multinomial como algoritmo de clasificación y la selección de palabras representativas mediante TF-IDF permitirá capturar la relevancia semántica de las palabras en cada reseña, lo que mejorará la capacidad del modelo para discernir la intensidad del sentimiento expresado en el texto. En conjunto, estas herramientas proporcionan una solución robusta y eficaz para el objetivo del negocio de analizar y clasificar las reseñas turísticas proporcionando un buen punto de partida para desarrolar el proyecto con el fin de identificar áreas de mejora y promover el turismo.

Para el segundo enfoque, ...

Para el tercer enfoque, ...

### **1.3 Principal organización beneficiada**

Para el alcance de este proyecto se decide enmarcar como principal beneficiario del proyecto a COTELCO (Asociación Hotelera y Turística de Colombia).

COTELCO desempeña un papel fundamental como representante y defensor de los intereses de la industria hotelera y turística en Colombia. Para este proyecto, COTELCO actúa como un facilitador y colaborador clave al proporcionar datos y conocimientos sobre el sector turístico colombiano, así como al participar en la implementación y validación del modelo analítico desarrollado. Además, COTELCO puede utilizar los resultados del proyecto para ofrecer recomendaciones y guías a sus miembros sobre cómo mejorar la calidad de sus servicios y la experiencia del turista, lo que a su vez puede beneficiar a toda la industria turística del país.

### **1.4 Detalle de la planeación de desarrollo**

A nivel de planeación se concretarón tres reuniones con el equipo de consulta de estadística con Karol Clavijo y Julio Gutiérrez, estas reuniones se harán de manera virtual por medio de google meet para validar el enfoque que le está dando el proyecto.

* Fecha primera reunión: Domingo 31 de Marzo
* Fecha segunda reunión: Jueves 04 de Abril
* Fecha tercera reunión: Sabado 06 de Abril

# **2. Entendimiento de los datos**
"""

# Cargar librerias

import pandas as pd
import numpy as np
np.random.seed(3301)
import pandas as pd
# Para preparar los datos
from sklearn.preprocessing import LabelEncoder
# Para crear el arbol de decisión
from sklearn.tree import DecisionTreeClassifier
# Para realizar la separación del conjunto de aprendizaje en entrenamiento y test.
from sklearn.model_selection import train_test_split
# Para evaluar el modelo
from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, accuracy_score
# Para búsqueda de hiperparámetros
from sklearn.model_selection import GridSearchCV
# Para la validación cruzada
from sklearn.model_selection import KFold
#Librerías para la visualización
import matplotlib.pyplot as plt
# Seaborn
import seaborn as sns
from sklearn import tree
from imblearn import under_sampling, over_sampling
from imblearn.over_sampling import SMOTE

# Cargar datos

df = pd.read_csv("tipo2_entrenamiento_estudiantes.csv")
display(df.head(5))

# Indentificar información general de los datos

print(df.shape)
print(df.dtypes)

"""Se compartieron datos de 7875 reseñas de turistas de los cuales tenemos una unica columna object "Review" en el que se encuentra toda la información de la reseña escrita. Por otro lado, hay una unica columna númerica "Class" que guarda el nivel de satisfación del turista con base en la reseña."""

# Analizar distribución de la variable numerica

df.describe()

# Analizar la completitud de los datos

print(df.isna().sum())

"""Se observa que no hay ningún dato nulo en el conjunto de reseñas compartido"""

# Analizar la unicidad de los datos

df.duplicated().sum()

"""Se observa que hay 73 reseñas duplicadas, es decir, 0.0093% de los datos se encuentran duplicados, por ende, se debe tratar estos datos."""

# Analizar validez de la variable númerica

df["Class"].value_counts().plot(kind="bar")
plt.title("Class")
plt.show()

"""Para la columna "Class" se observa un correcta validez al tener datos unicamente en las 5 clases que hay númeradas del 1 al 5.

#**3. Preparación de los datos**

La preparación de los datos se va a dividir en tres etapas:

*   Limpieza de los datos.
*   Tokenización.
*   Normalización (Vectorización).

### **3.1 Limpieza de los datos**

En esta parte lo que se hace es eliminar todo tipo de caracteres especiales y tener todas las palabras en minúscula

Primero se instalan todas las librerías y dependencias necesarias
"""

!pip install scikit-plot

!pip install contractions

# librería Natural Language Toolkit, usada para trabajar con textos
import nltk
# Punkt permite separar un texto en frases.
nltk.download('punkt')

nltk.download('stopwords')

nltk.download('wordnet')

# Instalación de librerias
import pandas as pd
import numpy as np
import sys
!{sys.executable} -m pip install pandas-profiling

import re, string, unicodedata
import contractions
import inflect
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer

from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import classification_report, confusion_matrix


from sklearn.base import BaseEstimator, ClassifierMixin

import matplotlib.pyplot as plt

"""Esta función quita todos los caracteres que no pertenezcan al código ASCII"""

def remove_non_ascii(words):
    """Remove non-ASCII characters from list of tokenized words"""
    new_words = []
    for word in words:
        if word is not None:
          new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')
          new_words.append(new_word)
    return new_words

"""Esta función pone todas las palabras en minúscula"""

def to_lowercase(words):
    """Convert all characters to lowercase from list of tokenized words"""
    words_to_lower = []
    for word in words:
       words_to_lower.append(word.lower())
    return words_to_lower

"""Esta función remueve todos los signos de puntuación"""

def remove_punctuation(words):
    """Remove punctuation from list of tokenized words"""
    new_words = []
    for word in words:
        if word is not None:
            new_word = re.sub(r'[^\w\s]', '', word)
            if new_word != '':
                new_words.append(new_word)
    return new_words

"""Esta función remueve todas las stopwords de la reseña, las stopwords son unas palabras que pertenecen a un grupo de palabras seleccionadas que no dan relevancia a las frases."""

def remove_stopwords(words):
    """Remove stop words from list of tokenized words"""

    stop_words = set(stopwords.words('spanish'))

    filtered_sentence = []

    for w in words:
      if w not in stop_words:
        filtered_sentence.append(w)

    return filtered_sentence

"""Esta función remplaza valores numpericos con su representación textual."""

def replace_numbers(words):
    """Replace all interger occurrences in list of tokenized words with textual representation"""
    p = inflect.engine()
    print(words)
    new_words = []
    for word in words:
        if word.isdigit():
            new_word = p.number_to_words(word)
            new_words.append(new_word)
            print("if " + new_word)
        else:
            new_words.append(word)
    return new_words

"""Esta función aplica a las palabras todas las funciones descritas anteriormente"""

def preprocessing(words):
    words = to_lowercase(words)
    words = replace_numbers(words)
    words = remove_punctuation(words)
    words = remove_non_ascii(words)
    words = remove_stopwords(words)

    return words

"""### **3.2 Tokenización**

Aplicamos la correccipon de contracciones y tokenizamos las palabras de Review (O sea dividimos el Review en una lista donde cada posición es una palabra del review)
"""

df_prep = df
df_prep['Review'] = df_prep['Review'].apply(contractions.fix) #Aplica la corrección de las contracciones

df_prep['words'] = df_prep['Review'].apply(word_tokenize)
df_prep.head()

"""Eliminamos palabras duplicadas"""

df_prep['words'].dropna()

df_prep.shape

df_prep['words'].info()

"""En esta parte aplicamos todas las funciones anteriores para limpiar los datos a cada lista de palabras"""

df_prep['words1']=df_prep['words'].apply(preprocessing) #Aplica la eliminación del ruido

"""Imprimimos el resultado"""

df_prep.head()

"""Vemos como se hizo exitosamente comparando la columna words1, con la columna words 2

### **3.3 Normalización**

Para el tema de la normalización o vectorización de las palabras,primero hacemos stem y lematizamos las palabras, esto con el fin de quitar prefijos o sufijos y lematizar las palabras.
"""

def stem_words(words):
    """Stem words in list of tokenized words"""
    stemmer = LancasterStemmer()
    stemmed_words = [stemmer.stem(word) for word in words]
    return stemmed_words

def lemmatize_verbs(words):
    """Lemmatize verbs in list of tokenized words"""
    lemmatizer = WordNetLemmatizer()
    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]
    return lemmatized_words

def stem_and_lemmatize(words):
    stems = stem_words(words)
    lemmas = lemmatize_verbs(words)
    return stems+lemmas

df_prep['words2'] = df_prep['words1'].apply(stem_and_lemmatize) #Aplica lematización y Eliminación de Prefijos y Sufijos.

df_prep["words2"].head()

"""Este es el resultado luego de aplicar el stem y lematización

### **3.4 Selección de campos**

Unimos la lista de palbras en un solo string para cada review.
"""

df_prep['words2'] = df_prep['words2'].apply(lambda x: ' '.join(map(str, x)))
df_prep

"""Creamos los conjuntos de datos."""

X_data, y_data = df_prep['words2'],df_prep['Class']
y_data

"""Aplicamos la vecotirzación, en este caso el count vectorizer y creamos un array con la matriz vectorizada para cada review"""

count = CountVectorizer()
X_count = count.fit_transform(X_data)
print(X_count.shape)
X_count.toarray()[0]

#**4. Creación del modelo**

# MODELO PARA SVM

# Importe de librerias necesarias

import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Dividir datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_count, y_data, test_size=0.2, random_state=42)

# Creación y entrenamiento del modelo SVM
svm_model = SVC(kernel='linear', C=1.0)
svm_model.fit(X_train, y_train)

# Evaluación del modelo
y_pred = svm_model.predict(X_test)

# Convierte la matriz dispersa a una matriz densa antes de usar argsort()
sorted_coef_index = svm_model.coef_.toarray()[0].argsort()

feature_names = count.get_feature_names_out()

# Obtener las 10 características más negativas y más positivas
print('Las 30 características más negativas: \n{}\n'.format(feature_names[sorted_coef_index[:30]]))
print('Las 30 características más positivas: \n{}'.format(feature_names[sorted_coef_index[:-31:-1]]))